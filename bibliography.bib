@article{Chierichetti2018,
abstract = {We study the question of fair clustering under the {\em disparate impact} doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the $k$-center and the $k$-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions---for instance a point may no longer be assigned to its nearest cluster center! En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms. While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow. We empirically quantify the value of fair clustering on real-world datasets with sensitive attributes.},
archivePrefix = {arXiv},
arxivId = {1802.05733},
author = {Chierichetti, Flavio and Kumar, Ravi and Lattanzi, Silvio and Vassilvitskii, Sergei},
eprint = {1802.05733},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chierichetti et al. - 2018 - Fair Clustering Through Fairlets.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
month = {feb},
pages = {5030--5038},
publisher = {Neural information processing systems foundation},
title = {{Fair Clustering Through Fairlets}},
url = {http://arxiv.org/abs/1802.05733},
volume = {2017-Decem},
year = {2018}
}
@article{Kleindessner2019,
abstract = {Given the widespread popularity of spectral clustering (SC) for partitioning graph data, we study a version of constrained SC in which we try to incorporate the fairness notion proposed by Chierichetti et al. (2017). According to this notion, a clustering is fair if every demographic group is approximately proportionally represented in each cluster. To this end, we develop variants of both normalized and unnormalized constrained SC and show that they help find fairer clusterings on both synthetic and real data. We also provide a rigorous theoretical analysis of our algorithms on a natural variant of the stochastic block model, where h groups have strong inter-group connectivity, but also exhibit a “natural” clustering structure which is fair. We prove that our algorithms can recover this fair clustering with high probability.},
author = {Kleindessner, Matth{\"{a}}us and Samadi, Samira and Awasthi, Pranjal and Morgenstern, Jamie},
file = {:C\:/Users/Bernhard/OneDrive - Fachhochschule Salzburg GmbH/Semester 4/Seminararbeit aus Informatik/Kleindessner2019.pdf:pdf},
issn = {23318422},
journal = {arXiv},
title = {{Guarantees for spectral clustering with fairness constraints}},
year = {2019}
}
@article{Schmidt2018,
abstract = {We study fair clustering problems as proposed by Chierichetti et al. (NIPS 2017). Here, points have a sensitive attribute and all clusters in the solution are required to be balanced with respect to it (to counteract any form of data-inherent bias). Previous algorithms for fair clustering do not scale well. We show how to model and compute so-called coresets for fair clustering problems, which can be used to significantly reduce the input data size. We prove that the coresets are composable and show how to compute them in a streaming setting. Furthermore, we propose a variant of Lloyd's algorithm that computes fair clusterings and extend it to a fair k-means++ clustering algorithm. We implement these algorithms and provide empirical evidence that the combination of our approximation algorithms and the coreset construction yields a scalable algorithm for fair k-means clustering.},
archivePrefix = {arXiv},
arxivId = {1812.10854},
author = {Schmidt, Melanie and Schwiegelshohn, Chris and Sohler, Christian},
eprint = {1812.10854},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Schmidt, Schwiegelshohn, Sohler - 2018 - Fair Coresets and Streaming Algorithms for Fair k-Means Clustering.pdf:pdf},
month = {dec},
title = {{Fair Coresets and Streaming Algorithms for Fair k-Means Clustering}},
url = {http://arxiv.org/abs/1812.10854},
year = {2018}
}
@inproceedings{Kawale2013,
abstract = {Constrained spectral clustering is a semi-supervised learning problem that aims at incorporating user-defined constraints in spectral clustering. Typically, there are two kinds of constraints: (i) must-link, and (ii) cannot-link. These constraints represent prior knowledge indicating whether two data objects should be in the same cluster or not; thereby aiding in clustering. In this paper, we propose a novel approach that uses convex subproblems to incorporate constraints in spectral clustering and co-clustering. In comparison to the prior state-of-art approaches, our approach presents a more natural way to incorporate constraints in the spectral methods and allows us to make a trade off between the number of satisfied constraints and the quality of partitions on the original graph. We use an L1 regularizer analogous to LASSO, often used in literature to induce sparsity, in order to control the number of constraints satisfied. Our approach can handle both must-link and cannot-link constraints, unlike a large number of previous approaches that mainly work on the former. Further, our formulation is based on the reduction to a convex subproblem which is relatively easy to solve using existing solvers. We test our proposed approach on real world datasets and show its effectiveness for both spectral clustering and co-clustering over the prior state-of-art.},
author = {Kawale, Jaya and Boley, Daniel},
booktitle = {Proceedings of the 2013 SIAM International Conference on Data Mining, SDM 2013},
doi = {10.1137/1.9781611972832.12},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kawale, Boley - 2013 - Constrained spectral clustering using L1 regularization.pdf:pdf},
isbn = {9781611972627},
pages = {103--111},
publisher = {Siam Society},
title = {{Constrained spectral clustering using L1 regularization}},
url = {https://experts.umn.edu/en/publications/constrained-spectral-clustering-using-l1-regularization-2},
year = {2013}
}
@article{Lei2013,
abstract = {We analyze the performance of spectral clustering for community extraction in stochastic block models. We show that, under mild conditions, spectral clustering applied to the adjacency matrix of the network can consistently recover hidden communities even when the order of the maximum expected degree is as small as $\log n$, with $n$ the number of nodes. This result applies to some popular polynomial time spectral clustering algorithms and is further extended to degree corrected stochastic block models using a spherical $k$-median spectral clustering method. A key component of our analysis is a combinatorial bound on the spectrum of binary random matrices, which is sharper than the conventional matrix Bernstein inequality and may be of independent interest.},
archivePrefix = {arXiv},
arxivId = {1312.2050},
author = {Lei, Jing and Rinaldo, Alessandro},
doi = {10.1214/14-AOS1274},
eprint = {1312.2050},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lei, Rinaldo - 2013 - Consistency of spectral clustering in stochastic block models.pdf:pdf},
journal = {Annals of Statistics},
keywords = {Network data,Sparsity,Spectral clustering,Stochastic block model},
month = {dec},
number = {1},
pages = {215--237},
publisher = {Institute of Mathematical Statistics},
title = {{Consistency of spectral clustering in stochastic block models}},
url = {http://arxiv.org/abs/1312.2050 http://dx.doi.org/10.1214/14-AOS1274},
volume = {43},
year = {2013}
}
@techreport{Yu2004,
abstract = {We consider data clustering problems where partial grouping is known a priori. We formulate such biased grouping problems as a constrained optimization problem, where structural properties of the data define the goodness of a grouping and partial grouping cues define the feasibility of a grouping. We enforce grouping smoothness and fairness on labeled data points so that sparse partial grouping information can be effectively propagated to the unlabeled data. Considering the normalized cuts criterion in particular, our formulation leads to a constrained eigenvalue problem. By generalizing the Rayleigh-Ritz theorem to projected matrices, we find the global optimum in the relaxed continuous domain by eigendecomposition, from which a near-global optimum to the discrete labeling problem can be obtained effectively. We apply our method to real image segmentation problems, where partial grouping priors can often be derived based on a crude spatial attentional map that binds places with common salient features or focuses on expected object locations. We demonstrate not only that it is possible to integrate both image structures and priors in a single grouping process, but also that objects can be segregated from the background without specific object knowledge.},
author = {Yu, Stella X. and Shi, Jianbo},
booktitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
doi = {10.1109/TPAMI.2004.1262179},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Yu, Shi - Unknown - Segmentation Given Partial Grouping Constraints.pdf:pdf},
issn = {01628828},
keywords = {Bias,Graph partitioning,Grouping,Image segmentation,Partially labeled classification,Semisupervised clustering,Spatial attention},
mendeley-groups = {SEM4 - Seminararbeit},
month = {feb},
number = {2},
pages = {173--183},
pmid = {15376893},
title = {{Segmentation Given Partial Grouping Constraints}},
url = {http://ieeexplore.ieee.org/xpl/tocresult.jsp?isNumber=28218&puNumber=34},
volume = {26},
year = {2004}
}
@techreport{Eriksson2011,
abstract = {Indisputably Normalized Cuts is one of the most popular segmentation algorithms in computer vision. It has been applied to a wide range of segmentation tasks with great success. A number of extensions to this approach have also been proposed, ones that can deal with multiple classes or that can incorporate a priori information in the form of grouping constraints. However, what is common for all these suggested methods is that they are noticeably limited and can only address segmentation problems on a very specific form. In this paper, we present a reformulation of Normalized Cut segmentation that in a unified way can handle all types of linear equality constraints for an arbitrary number of classes. This is done by restating the problem and showing how linear constraints can be enforced exactly through duality. This allows us to add group priors, for example , that certain pixels should belong to a given class. In addition, it provides a principled way to perform multi-class segmentation for tasks like interactive segmentation. The method has been tested on real data with convincing results.},
author = {Eriksson, Anders P and Olsson, Carl and Kahl, Fredrik},
booktitle = {Journal of Mathematical Imaging and Vision},
doi = {10.1007/s10851-010-0223-5},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Eriksson, Olsson, Kahl - Unknown - Normalized Cuts Revisited A Reformulation for Segmentation with Linear Grouping Constraints.pdf:pdf},
issn = {09249907},
keywords = {Image segmentation,Linear grouping constraints,Normalized Cuts,Optimization},
month = {jan},
number = {1},
pages = {45--61},
publisher = {Springer},
title = {{Normalized Cuts Revisited: A Reformulation for Segmentation with Linear Grouping Constraints}},
url = {https://link.springer.com/article/10.1007/s10851-010-0223-5},
volume = {39},
year = {2011}
}
@article{VonLuxburg2007,
abstract = {In recent years, spectral clustering has become one of the most popular modern clustering algorithms. It is simple to implement, can be solved efficiently by standard linear algebra software, and very often outperforms traditional clustering algorithms such as the k-means algorithm. On the first glance spectral clustering appears slightly mysterious, and it is not obvious to see why it works at all and what it really does. The goal of this tutorial is to give some intuition on those questions. We describe different graph Laplacians and their basic properties, present the most common spectral clustering algorithms, and derive those algorithms from scratch by several different approaches. Advantages and disadvantages of the different spectral clustering algorithms are discussed.},
archivePrefix = {arXiv},
arxivId = {0711.0189},
author = {von Luxburg, Ulrike},
doi = {10.1007/s11222-007-9033-z},
eprint = {0711.0189},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/von Luxburg - 2007 - A Tutorial on Spectral Clustering.pdf:pdf},
issn = {09603174},
journal = {Statistics and Computing},
keywords = {Graph Laplacian,Spectral clustering},
month = {nov},
number = {4},
pages = {395--416},
title = {{A Tutorial on Spectral Clustering}},
url = {http://arxiv.org/abs/0711.0189},
volume = {17},
year = {2007}
}
@techreport{Xu2010,
abstract = {Normalized Cut is a widely used technique for solving a variety of problems. Although finding the optimal normalized cut has proven to be NP-hard, spectral relaxations can be applied and the problem of minimizing the normalized cut can be approximately solved using eigen-computations. However, it is a challenge to incorporate prior information in this approach. In this paper, we express prior knowledge by linear constraints on the solution, with the goal of minimizing the normalized cut criterion with respect to these constraints. We develop a fast and effective algorithm that is guaranteed to converge. Convincing results are achieved on image segmentation tasks, where the prior knowledge is given as the grouping information of features.},
author = {Xu, Linli and Li, Wenye and Schuurmans, Dale},
doi = {10.1109/cvpr.2009.5206561},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Xu, Li, Schuurmans - Unknown - Fast Normalized Cut with Linear Constraints.pdf:pdf},
month = {mar},
pages = {2866--2873},
publisher = {Institute of Electrical and Electronics Engineers (IEEE)},
title = {{Fast Normalized Cut with Linear Constraints}},
year = {2010}
}
@article{Bera2019,
abstract = {We study the problem of finding low-cost Fair Clusterings in data where each data point may belong to many protected groups. Our work significantly generalizes the seminal work of Chierichetti et.al. (NIPS 2017) as follows. - We allow the user to specify the parameters that define fair representation. More precisely, these parameters define the maximum over- and minimum under-representation of any group in any cluster. - Our clustering algorithm works on any $\ell_p$-norm objective (e.g. $k$-means, $k$-median, and $k$-center). Indeed, our algorithm transforms any vanilla clustering solution into a fair one incurring only a slight loss in quality. - Our algorithm also allows individuals to lie in multiple protected groups. In other words, we do not need the protected groups to partition the data and we can maintain fairness across different groups simultaneously. Our experiments show that on established data sets, our algorithm performs much better in practice than what our theoretical results suggest.},
archivePrefix = {arXiv},
arxivId = {1901.02393},
author = {Bera, Suman K. and Chakrabarty, Deeparnab and Flores, Nicolas J. and Negahbani, Maryam},
eprint = {1901.02393},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bera et al. - 2019 - Fair Algorithms for Clustering.pdf:pdf},
journal = {arXiv},
month = {jan},
publisher = {arXiv},
title = {{Fair Algorithms for Clustering}},
url = {http://arxiv.org/abs/1901.02393},
year = {2019}
}
@article{Barlow1989,
abstract = {What use can the brain make of the massive flow of sensory information that occurs without any associated rewards or punishments? This question is reviewed in the light of connectionist models of unsupervised learning and some older ideas, namely the cognitive maps and working models of Tolman and Craik, and the idea that redundancy is important for understanding perception (Attneave 1954), the physiology of sensory pathways (Barlow 1959), and pattern recognition (Watanabe 1960). It is argued that (1) The redundancy of sensory messages provides the knowledge incorporated in the maps or models. (2) Some of this knowledge can be obtained by observations of mean, variance, and covariance of sensory messages, and perhaps also by a method called “minimum entropy coding.” (3) Such knowledge may be incorporated in a model of “what usually happens” with which incoming messages are automatically compared, enabling unexpected discrepancies to be immediately identified. (4) Knowledge of the sort incorporated into such a filter is a necessary prerequisite of ordinary learning, and a representation whose elements are independent makes it possible to form associations with logical functions of the elements, not just with the elements themselves.},
author = {Barlow, H.B.},
doi = {10.1162/neco.1989.1.3.295},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Barlow - 1989 - Unsupervised Learning.pdf:pdf},
issn = {0899-7667},
journal = {Neural Computation},
month = {sep},
number = {3},
pages = {295--311},
publisher = {MIT Press - Journals},
title = {{Unsupervised Learning}},
url = {http://direct.mit.edu/neco/article-pdf/1/3/295/811863/neco.1989.1.3.295.pdf},
volume = {1},
year = {1989}
}
@techreport{Ng2001,
abstract = {Despite many empirical successes of spectral clustering methods- Algorithms that cluster points using eigenvectors of matrices derived from the data-there are several unresolved issues. First, there are a wide variety of algorithms that use the eigenvectors in slightly different ways. Second, many of these algorithms have 110 proof that they will actually compute a reasonable clustering. T11 this paper, we present a simple spectral clustering algorithm that can be implemented using a few lines of Matlab. Using tools from matrix perturbation theory, we analyze the algorithm, and give conditions under which it can be expected to do well. We also show surprisingly good experimental results 011 a number of challenging clustering problems.},
author = {Ng, Andrew Y. and Jordan, Michael I. and Weiss, Yair},
booktitle = {Advances in Neural Information Processing Systems},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ng, Jordan - 2001 - On Spectral Clustering Analysis and an algorithm.pdf:pdf},
isbn = {0262042088},
issn = {10495258},
title = {{On spectral clustering: Analysis and an algorithm}},
url = {https://ai.stanford.edu/$\sim$ang/papers/nips01-spectral.pdf},
volume = {14},
year = {2002}
}
@article{Likas2003,
abstract = {We present the global k-means algorithm which is an incremental approach to clustering that dynamically adds one cluster center at a time through a deterministic global search procedure consisting of N (with N being the size of the data set) executions of the k-means algorithm from suitable initial positions. We also propose modifications of the method to reduce the computational load without significantly affecting solution quality. The proposed clustering methods are tested on well-known data sets and they compare favorably to the k-means algorithm with random restarts. {\textcopyright} 2002 Pattern Recognition Society. Published by Elsevier Science Ltd. All rights reserved.},
author = {Likas, Aristidis and Vlassis, Nikos and {J. Verbeek}, Jakob},
doi = {10.1016/S0031-3203(02)00060-2},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Likas, Vlassis, J. Verbeek - 2003 - The global k-means clustering algorithm.pdf:pdf},
issn = {00313203},
journal = {Pattern Recognition},
keywords = {Clustering,Data mining,Global optimatization,k-Means algorithm,k-d Trees},
month = {feb},
number = {2},
pages = {451--461},
publisher = {Elsevier Ltd},
title = {{The global k-means clustering algorithm}},
volume = {36},
year = {2003}
}
@article{Nascimento2011,
abstract = {Graph clustering is an area in cluster analysis that looks for groups of related vertices in a graph. Due to its large applicability, several graph clustering algorithms have been proposed in the last years. A particular class of graph clustering algorithms is known as spectral clustering algorithms. These algorithms are mostly based on the eigen-decomposition of Laplacian matrices of either weighted or unweighted graphs. This survey presents different graph clustering formulations, most of which based on graph cut and partitioning problems, and describes the main spectral clustering algorithms found in literature that solve these problems. {\textcopyright} 2011 Elsevier B.V. All rights reserved.},
author = {Nascimento, Mari{\'{a}} C.V. and {De Carvalho}, Andr{\'{e}} C.P.L.F.},
doi = {10.1016/j.ejor.2010.08.012},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nascimento, De Carvalho - 2011 - Spectral methods for graph clustering - A survey.pdf:pdf},
issn = {03772217},
journal = {European Journal of Operational Research},
keywords = {Min-cut,Modularity,Ratio cut,Spectral clustering,ncut},
month = {jun},
number = {2},
pages = {221--231},
publisher = {Elsevier B.V.},
title = {{Spectral methods for graph clustering - A survey}},
volume = {211},
year = {2011}
}
@article{Shi2000,
abstract = {We propose a novel approach for solving the perceptual grouping problem in vision. Rather than focusing on local features and their consistencies in the image data, our approach aims at extracting the global impression of an image. We treat image segmentation as a graph partitioning problem and propose a novel global criterion, the normalized cut, for segmenting the graph. The normalized cut criterion measures both the total dissimilarity between the different groups as well as the total similarity within the groups. We show that an efficient computational technique based on a generalized eigenvalue problem can be used to optimize this criterion. We have applied this approach to segmenting static images, as well as motion sequences, and found the results to be very encouraging.},
author = {Shi, Jianbo and Malik, Jitendra},
doi = {10.1109/34.868688},
file = {:C\:/Users/Bernhard/Desktop/00868688.pdf:pdf},
issn = {01628828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {8},
pages = {888--905},
title = {{Normalized cuts and image segmentation}},
volume = {22},
year = {2000}
}
@article{Holland1983,
abstract = {A stochastic model is proposed for social networks in which the actors in a network are partItIoned mto subgroups called blocks. The model provides a stochastrc generalization of the blockmodel. Estimation techniques are developed for the special case of a single relation social network, with blocks specified D prrorr. An extension of the model allows for tendencies toward reciprocation of ties beyond those explained by the partition. The extended model prowdes a one degree-of-freedom test of the model. A numerical example from the social network hterature 1s used to illustrate the methods.},
author = {Holland, Paul W. and Laskey, Kathryn Blackmond and Leinhardt, Samuel},
doi = {10.1016/0378-8733(83)90021-7},
file = {:C\:/Users/Bernhard/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Holland, Blackmond, Leinhardt - 1983 - STOCHASTIC BLOCKMODELS FIRST STEPS Educational Testing Seroice.pdf:pdf},
issn = {03788733},
journal = {Social Networks},
month = {jun},
number = {2},
pages = {109--137},
publisher = {North-Holland},
title = {{Stochastic blockmodels: First steps}},
volume = {5},
year = {1983}
}
